{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right\">\n",
    "  <img src=\"../images/dtlogo.png\" alt=\"Logo\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª 02 - Stereo Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 : Intrinsic calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a wrapper around the [tutorial](https://docs.luxonis.com/en/latest/pages/calibration/) in the Luxonis Documentation. <br>\n",
    "Print the calibration board provided in the documentation, plug the OAK-D to the computer and run:<br><br>\n",
    "`dts oakd calibrate`<br><br>\n",
    " The DepthAI calibration program will start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the basic implementation notebook and check the disparity image. Is it improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "%matplotlib notebook\n",
    "# first, import all necessary modules\n",
    "from pathlib import Path\n",
    "\n",
    "import blobconverter\n",
    "import cv2\n",
    "import depthai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /tmp/.cache/blobconverter/mobilenet-ssd_openvino_2021.3_6shave.blob...\n",
      "[==================================================]\n",
      "Done\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to find device after booting, error message: X_LINK_DEVICE_NOT_FOUND",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eb0b6c8323ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Pipeline is now finished, and we need to find an available device to run our pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# we are using context manager here that will dispose the device after we stop using it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mdepthai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# From this point, the Device will be in \"running\" mode and will start sending data via XLink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to find device after booting, error message: X_LINK_DEVICE_NOT_FOUND"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pipeline tells DepthAI what operations to perform when running - you define all of the resources used and flows here\n",
    "pipeline = depthai.Pipeline()\n",
    "\n",
    "# First, we want the Color camera as the output\n",
    "cam_rgb = pipeline.createColorCamera()\n",
    "cam_rgb.setPreviewSize(300, 300)  # 300x300 will be the preview frame size, available as 'preview' output of the node\n",
    "cam_rgb.setInterleaved(False)\n",
    "\n",
    "# Next, we want a neural network that will produce the detections\n",
    "detection_nn = pipeline.createMobileNetDetectionNetwork()\n",
    "# Blob is the Neural Network file, compiled for MyriadX. It contains both the definition and weights of the model\n",
    "# We're using a blobconverter tool to retreive the MobileNetSSD blob automatically from OpenVINO Model Zoo\n",
    "detection_nn.setBlobPath(str(blobconverter.from_zoo(name='mobilenet-ssd', shaves=6)))\n",
    "# Next, we filter out the detections that are below a confidence threshold. Confidence can be anywhere between <0..1>\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "# Next, we link the camera 'preview' output to the neural network detection input, so that it can produce detections\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# XLinkOut is a \"way out\" from the device. Any data you want to transfer to host need to be send via XLink\n",
    "xout_rgb = pipeline.createXLinkOut()\n",
    "# For the rgb camera output, we want the XLink stream to be named \"rgb\"\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "# Linking camera preview to XLink input, so that the frames will be sent to host\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "# The same XLinkOut mechanism will be used to receive nn results\n",
    "xout_nn = pipeline.createXLinkOut()\n",
    "xout_nn.setStreamName(\"nn\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "# Pipeline is now finished, and we need to find an available device to run our pipeline\n",
    "# we are using context manager here that will dispose the device after we stop using it\n",
    "with depthai.Device(pipeline) as device:\n",
    "    # From this point, the Device will be in \"running\" mode and will start sending data via XLink\n",
    "\n",
    "    # To consume the device results, we get two output queues from the device, with stream names we assigned earlier\n",
    "    q_rgb = device.getOutputQueue(\"rgb\")\n",
    "    q_nn = device.getOutputQueue(\"nn\")\n",
    "\n",
    "    # Here, some of the default values are defined. Frame will be an image from \"rgb\" stream, detections will contain nn results\n",
    "    frame = None\n",
    "    detections = []\n",
    "\n",
    "    # Since the detections returned by nn have values from <0..1> range, they need to be multiplied by frame width/height to\n",
    "    # receive the actual position of the bounding box on the image\n",
    "    def frameNorm(frame, bbox):\n",
    "        normVals = np.full(len(bbox), frame.shape[0])\n",
    "        normVals[::2] = frame.shape[1]\n",
    "        return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)\n",
    "\n",
    "\n",
    "    # Main host-side application loop\n",
    "    while True:\n",
    "        # we try to fetch the data from nn/rgb queues. tryGet will return either the data packet or None if there isn't any\n",
    "        in_rgb = q_rgb.tryGet()\n",
    "        in_nn = q_nn.tryGet()\n",
    "\n",
    "        if in_rgb is not None:\n",
    "            # If the packet from RGB camera is present, we're retrieving the frame in OpenCV format using getCvFrame\n",
    "            frame = in_rgb.getCvFrame()\n",
    "\n",
    "        if in_nn is not None:\n",
    "            # when data from nn is received, we take the detections array that contains mobilenet-ssd results\n",
    "            detections = in_nn.detections\n",
    "\n",
    "        if frame is not None:\n",
    "            for detection in detections:\n",
    "                # for each bounding box, we first normalize it to match the frame size\n",
    "                bbox = frameNorm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n",
    "                # and then draw a rectangle on the frame to show the actual result\n",
    "                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "            # After all the drawing is finished, we show the frame on the screen\n",
    "            cv2.imshow(\"preview\", frame)\n",
    "\n",
    "        # at any time, you can press \"q\" and exit the main loop, therefore exiting the program itself\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è ***WARNING:***\n",
    "- DO NOT CHANGE THE ORDER OF THE FOLLOWING CODE CELLS\n",
    "- DO NOT CHANGE THE NAME OF THE FOLLOWING FUNCTIONS\n",
    "- DO NOT CHANGE THE ARGUMENTS OF THE FUNCTIONS\n",
    "- DO NOT CREATE NEW CODE CELLS, THEY WILL NOT BE CONSIDERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... logging to /tmp/.ros/log/e2460568-d74a-11eb-8dca-0242ac110002/roslaunch-429c6e7e239a-2378.log\n",
      "Checking log directory for disk usage. This may take a while.\n",
      "Press Ctrl-C to interrupt\n",
      "Done checking log file disk usage. Usage is <1GB.\n",
      "\u001b]2;roscore\u0007\n",
      "\u001b[1mstarted roslaunch server http://172.17.0.2:43619/\u001b[0m\n",
      "ros_comm version 1.15.11\n",
      "\n",
      "\n",
      "SUMMARY\n",
      "========\n",
      "\n",
      "PARAMETERS\n",
      " * /rosdistro: noetic\n",
      " * /rosversion: 1.15.11\n",
      "\n",
      "NODES\n",
      "\n",
      "auto-starting new master\n",
      "\u001b[1mprocess[master]: started with pid [2386]\u001b[0m\n",
      "\u001b[1mROS_MASTER_URI=http://172.17.0.2:11311/\u001b[0m\n",
      "\u001b]2;roscore http://172.17.0.2:11311/\u0007\n",
      "\u001b[1msetting /run_id to e2460568-d74a-11eb-8dca-0242ac110002\u001b[0m\n",
      "\u001b[1mprocess[rosout-1]: started with pid [2404]\u001b[0m\n",
      "started core service [/rosout]\n",
      "^C\n",
      "[rosout-1] killing on exit\n"
     ]
    }
   ],
   "source": [
    "!roscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import copy\n",
    "import rospy\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "from sensor_msgs.msg import CompressedImage, Image, CameraInfo\n",
    "from sensor_msgs.srv import SetCameraInfo, SetCameraInfoResponse\n",
    "\n",
    "from duckietown.dtros import DTROS, NodeType, TopicType, DTParam, ParamType\n",
    "\n",
    "import depthai as dai\n",
    "\n",
    "USE_CV_BRIDGE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creatirg the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init the oakd module and topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You cannot instantiate two objects of type DTROS",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-45589ad3ab0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;31m# initialize the node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0mcamera_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOAKDCameraNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m     \u001b[0mcamera_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;31m# keep the node alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-45589ad3ab0b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Initialize the DTROS parent class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         super(OAKDCameraNode, self).__init__(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mnode_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'camera'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mnode_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNodeType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDRIVER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/code/catkin_ws/src/dt-ros-commons/packages/duckietown/include/duckietown/dtros/dtros.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_name, node_type, help, dt_ghost)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# configure singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__instance__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You cannot instantiate two objects of type DTROS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__instance__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNodeType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You cannot instantiate two objects of type DTROS"
     ]
    }
   ],
   "source": [
    "# Do not change the nameing of the functions/classes\n",
    "\n",
    "class OAKDCameraNode(DTROS):\n",
    "    \"\"\"Handles the imagery.\n",
    "\n",
    "    The node handles the image stream, initializing it, publishing frames\n",
    "    according to the required frequency and stops it at shutdown.\n",
    "\n",
    "    Note that only one instance of this class should be used at a time.\n",
    "    If another node tries to start an instance while this node is running,\n",
    "    it will likely fail with an `Out of resource` exception.\n",
    "\n",
    "    The configuration parameters can be changed dynamically while the node is running via\n",
    "    `rosparam set` commands.\n",
    "\n",
    "    Configuration:\n",
    "        ~framerate (:obj:`float`): The camera image acquisition framerate, default is 20.0 fps\n",
    "        ~res_w (:obj:`int`): The desired width of the acquired image, default is 640px\n",
    "        ~res_h (:obj:`int`): The desired height of the acquired image, default is 480px\n",
    "\n",
    "    Publisher:\n",
    "        ~image/compressed (:obj:`CompressedImage`): The acquired camera images\n",
    "        ~camera_info (:obj:`CameraInfo`): The camera parameters\n",
    "\n",
    "    Service:\n",
    "        ~set_camera_info:\n",
    "            Saves a provided camera info\n",
    "            to `/data/config/calibrations/camera_intrinsic/HOSTNAME.yaml`.\n",
    "\n",
    "            input:\n",
    "                camera_info (`CameraInfo`): The camera information to save\n",
    "\n",
    "            outputs:\n",
    "                success (`bool`): `True` if the call succeeded\n",
    "                status_message (`str`): Used to give details about success\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize the DTROS parent class\n",
    "        super(OAKDCameraNode, self).__init__(\n",
    "            node_name='camera',\n",
    "            node_type=NodeType.DRIVER,\n",
    "            help=\"Camera driver for reading and publishing OAK-D images\"\n",
    "        )\n",
    "        # Add the node parameters to the parameters dictionary and load their default values\n",
    "        self._res_w = DTParam(\n",
    "            '~res_w',\n",
    "            param_type=ParamType.INT,\n",
    "            default=640,\n",
    "            help=\"Horizontal resolution (width) of the produced image frames.\"\n",
    "        )\n",
    "        self._res_h = DTParam(\n",
    "            '~res_h',\n",
    "            param_type=ParamType.INT,\n",
    "            default=400,\n",
    "            help=\"Vertical resolution (height) of the produced image frames.\"\n",
    "        )\n",
    "        self._framerate = DTParam(\n",
    "            '~framerate',\n",
    "            param_type=ParamType.INT,\n",
    "            default=20,\n",
    "            help=\"Framerate at which images frames are produced\"\n",
    "        )\n",
    "\n",
    "        # define parameters\n",
    "        self._framerate.register_update_callback(self.parameters_updated)\n",
    "        self._res_w.register_update_callback(self.parameters_updated)\n",
    "        self._res_h.register_update_callback(self.parameters_updated)\n",
    "\n",
    "        # intrinsic calibration\n",
    "        veh_name = rospy.get_namespace().rstrip('/')\n",
    "        self.frame_ids = {\n",
    "            'left': veh_name + '/camera_left_frame',\n",
    "            'right': veh_name + '/camera_right_frame',\n",
    "            'rgb': veh_name + '/camera_frame',\n",
    "            'disparity': veh_name + '/camera_left_frame'  # disparity/depth are in left camera frame\n",
    "        }\n",
    "        self.cali_file_folder = '/data/config/calibrations/camera_intrinsic/oakd/'\n",
    "\n",
    "        cali_files = {\n",
    "            'left': self.cali_file_folder + veh_name + '_left.yaml',\n",
    "            'right': self.cali_file_folder + veh_name + '_right.yaml',\n",
    "            'rgb': self.cali_file_folder + veh_name + '.yaml'\n",
    "        }\n",
    "\n",
    "        for cam, cali_file in cali_files.items():\n",
    "            # locate calibration yaml file or use the default otherwise\n",
    "            if not os.path.isfile(cali_file):\n",
    "                self.logwarn('Calibration not found: %s.\\n Using default instead.' % cali_file)\n",
    "                cali_files[cam] = None\n",
    "\n",
    "        # load the calibration file\n",
    "        self.original_camera_infos = dict()\n",
    "        for cam, cali_file in cali_files.items():\n",
    "            if cali_file is None:\n",
    "                self.original_camera_infos[cam] = self.default_intrinsics(cam)\n",
    "                cali_file = '(default)'\n",
    "            else:\n",
    "                self.original_camera_infos[cam] = self.load_camera_info(cali_file)\n",
    "            self.log('For camera %s, using calibration file: %s' % (cam, cali_file))\n",
    "            self.original_camera_infos[cam].header.frame_id = self.frame_ids[cam]\n",
    "\n",
    "        self.current_camera_infos = copy.deepcopy(self.original_camera_infos)\n",
    "        for cam in cali_files.keys():\n",
    "            self.update_camera_params(cam)\n",
    "\n",
    "        if USE_CV_BRIDGE:\n",
    "            # create cv bridge\n",
    "            self._bridge = CvBridge()\n",
    "\n",
    "        # OAK-D interface\n",
    "        self._pipeline = None\n",
    "        self._cam_left = None\n",
    "        self._cam_right = None\n",
    "        self._cam_rgb = None\n",
    "        self._disparity = None\n",
    "        self._manip_rgb = None\n",
    "        self._xout_left = None\n",
    "        self._xout_right = None\n",
    "        self._xout_manip_rgb = None\n",
    "        self._xout_disparity = None\n",
    "        self._device = None\n",
    "        self._queue_left = None\n",
    "        self._queue_right = None\n",
    "        self._queue_rgb = None\n",
    "        self._queue_disparity = None\n",
    "\n",
    "        # Setup publishers\n",
    "        self._is_stopped = False\n",
    "        self._worker = None\n",
    "\n",
    "        self.pub_images = {\n",
    "            'left': rospy.Publisher(\n",
    "                '~image_left/compressed',\n",
    "                CompressedImage,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='The stream of JPEG compressed images from OAK-D left camera'),\n",
    "            'right': rospy.Publisher(\n",
    "                '~image_right/compressed',\n",
    "                CompressedImage,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='The stream of JPEG compressed images from OAK-D right camera'),\n",
    "            'rgb': rospy.Publisher(\n",
    "                '~image/compressed',\n",
    "                CompressedImage,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='The stream of JPEG compressed images from OAK-D RGB camera'),\n",
    "            'disparity': rospy.Publisher(\n",
    "                '~image_disparity/compressed',\n",
    "                CompressedImage,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='Disparity image from OAK-D stereo'),\n",
    "            'all': rospy.Publisher(\n",
    "                '~all_images',\n",
    "                OAKDImages,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='All images (left, right, RGB, disparity) from OAK-D, synchronized')\n",
    "        }\n",
    "\n",
    "        self.pub_camera_infos = {\n",
    "            'left': rospy.Publisher(\n",
    "                '~left_camera_info',\n",
    "                CameraInfo,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='Left camera calibration information, the message content is fixed'),\n",
    "            'right': rospy.Publisher(\n",
    "                '~right_camera_info',\n",
    "                CameraInfo,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='Right camera calibration information, the message content is fixed'),\n",
    "            'rgb': rospy.Publisher(\n",
    "                '~camera_info',\n",
    "                CameraInfo,\n",
    "                queue_size=1,\n",
    "                dt_topic_type=TopicType.DRIVER,\n",
    "                dt_help='RGB camera calibration information, the message content is fixed')\n",
    "        }\n",
    "\n",
    "        # Setup service (for camera_calibration)\n",
    "        self.srv_set_camera_infos = {\n",
    "            'left': rospy.Service(\n",
    "                '~set_left_camera_info',\n",
    "                SetCameraInfo,\n",
    "                self.srv_set_left_camera_info_cb),\n",
    "            'right': rospy.Service(\n",
    "                '~set_right_camera_info',\n",
    "                SetCameraInfo,\n",
    "                self.srv_set_right_camera_info_cb),\n",
    "            'rgb': rospy.Service(\n",
    "                '~camera_info',\n",
    "                SetCameraInfo,\n",
    "                self.srv_camera_info_cb)\n",
    "        }\n",
    "        # ---\n",
    "        self.log('[OAKDCameraNode]: Initialized.')\n",
    "\n",
    "    @property\n",
    "    def is_stopped(self):\n",
    "        return self._is_stopped\n",
    "\n",
    "    def parameters_updated(self):\n",
    "        self.stop()\n",
    "        for cam in self.original_camera_infos.keys():\n",
    "            self.update_camera_params(cam)\n",
    "        self.start()\n",
    "\n",
    "    def publish(self, image_msgs):\n",
    "        # add time to messages\n",
    "        stamp = rospy.Time.now()\n",
    "        valid_msgs = set()\n",
    "\n",
    "        for camera_name, image_msg in image_msgs.items():\n",
    "            if type(image_msg) is not CompressedImage:\n",
    "                continue\n",
    "\n",
    "            valid_msgs.add(camera_name)\n",
    "            image_msg.header.stamp = stamp\n",
    "            # update camera frame\n",
    "            image_msg.header.frame_id = self.frame_ids[camera_name]\n",
    "            # publish image\n",
    "            self.pub_images[camera_name].publish(image_msg)\n",
    "            # Save modified image message\n",
    "            image_msgs[camera_name] = image_msg\n",
    "            # publish camera info\n",
    "            if camera_name in ('left', 'right', 'rgb'):\n",
    "                self.current_camera_infos[camera_name].header.stamp = stamp\n",
    "                self.pub_camera_infos[camera_name].publish(\n",
    "                    self.current_camera_infos[camera_name])\n",
    "\n",
    "        if valid_msgs == set(['left', 'right', 'rgb', 'disparity']):\n",
    "            # All images were successfully read\n",
    "            all_images = OAKDImages()\n",
    "            all_images.left_image = image_msgs['left']\n",
    "            all_images.right_image = image_msgs['right']\n",
    "            all_images.rgb_image = image_msgs['rgb']\n",
    "            all_images.disparity_image = image_msgs['disparity']\n",
    "            self.pub_images['all'].publish(all_images)\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Begins the camera capturing.\n",
    "        \"\"\"\n",
    "        self.log('Start capturing.')\n",
    "        # ---\n",
    "        try:\n",
    "            self.setup()\n",
    "            # run camera thread\n",
    "            self._worker = Thread(target=self.run)\n",
    "            self._worker.start()\n",
    "        except StopIteration:\n",
    "            self.log('Exception thrown.')\n",
    "\n",
    "    def stop(self):\n",
    "        self.loginfo('Stopping OAK-D...')\n",
    "        self._is_stopped = True\n",
    "        # wait for the camera thread to finish\n",
    "        if self._worker is not None:\n",
    "            self._worker.join()\n",
    "            time.sleep(2)\n",
    "        self._worker = None\n",
    "        # release resources\n",
    "        self.release()\n",
    "        time.sleep(2)\n",
    "        self._is_stopped = False\n",
    "        self.loginfo('OAK-D stopped.')\n",
    "\n",
    "    def setup(self):\n",
    "        if self._device is not None:\n",
    "            # Close device if it was previously initialized\n",
    "            self._device.close()\n",
    "            time.sleep(2)\n",
    "\n",
    "        ###### OAK-D Configuration ######\n",
    "\n",
    "        # Start defining a pipeline\n",
    "        self._pipeline = dai.Pipeline()\n",
    "\n",
    "        # LEFT CAMERA\n",
    "        self._cam_left = self._pipeline.createMonoCamera()\n",
    "        self._cam_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "        self._cam_left.setResolution(\n",
    "            dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "        # RIGHT CAMERA\n",
    "        self._cam_right = self._pipeline.createMonoCamera()\n",
    "        self._cam_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "        self._cam_right.setResolution(\n",
    "            dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "        # RGB CAMERA\n",
    "        self._cam_rgb = self._pipeline.createColorCamera()\n",
    "        self._cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "        self._cam_rgb.setResolution(\n",
    "            dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "        self._cam_rgb.setColorOrder(\n",
    "            dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "        self._cam_rgb.setInterleaved(False)\n",
    "        self._cam_rgb.setPreviewSize(640, 400)\n",
    "\n",
    "        # DISPARITY\n",
    "        self._disparity = self._pipeline.createStereoDepth()\n",
    "        self._disparity.setConfidenceThreshold(150)\n",
    "        self._disparity.setMedianFilter(\n",
    "            dai.StereoDepthProperties.MedianFilter.KERNEL_7x7)\n",
    "        self._disparity.setLeftRightCheck(False)\n",
    "        self._disparity.setExtendedDisparity(False)  # True = better for short-range\n",
    "        self._disparity.setSubpixel(False)  # True = better for long-range\n",
    "        # self._disparity.setDepthAlign(dai.CameraBoardSocket.LEFT)\n",
    "        self._disparity.setRectifyMirrorFrame(False)\n",
    "\n",
    "        # IMAGE MANIPULATION\n",
    "        self._manip_rgb = self._pipeline.createImageManip()\n",
    "\n",
    "        for manip in [self._manip_rgb]:\n",
    "            manip.initialConfig.setResize(640, 400)\n",
    "            manip.setWaitForConfigInput(False)\n",
    "            manip.setKeepAspectRatio(True)\n",
    "\n",
    "        # OUTPUT LINKS\n",
    "        self._xout_left = self._pipeline.createXLinkOut()\n",
    "        self._xout_right = self._pipeline.createXLinkOut()\n",
    "        self._xout_manip_rgb = self._pipeline.createXLinkOut()\n",
    "        self._xout_disparity = self._pipeline.createXLinkOut()\n",
    "\n",
    "        self._xout_left.setStreamName('left_stream')\n",
    "        self._xout_right.setStreamName('right_stream')\n",
    "        self._xout_manip_rgb.setStreamName('rgb_stream')\n",
    "        self._xout_disparity.setStreamName('disparity_stream')\n",
    "\n",
    "        # Raw RGB image --> Image manipulation module\n",
    "        self._cam_rgb.preview.link(self._manip_rgb.inputImage)\n",
    "        # Disparity --> link the raw left & right images as input to stereo\n",
    "        self._cam_left.out.link(self._disparity.left)\n",
    "        self._cam_right.out.link(self._disparity.right)\n",
    "        self._disparity.disparity.link(self._xout_disparity.input)\n",
    "        self._disparity.rectifiedLeft.link(self._xout_left.input)\n",
    "        self._disparity.rectifiedRight.link(self._xout_right.input)\n",
    "        # Image manipulation module --> public outputs\n",
    "        self._manip_rgb.out.link(self._xout_manip_rgb.input)\n",
    "\n",
    "        # Pipeline is defined, now we can connect to the device\n",
    "        self._device = dai.Device(self._pipeline, usb2Mode=False)\n",
    "        self._device.setLogLevel(dai.LogLevel.DEBUG)\n",
    "\n",
    "        # True specifies blocking and False overwriting of oldest messages\n",
    "        self._queue_left = self._device.getOutputQueue(\n",
    "            name='left_stream', maxSize=1, blocking=False)\n",
    "        self._queue_right = self._device.getOutputQueue(\n",
    "            name='right_stream', maxSize=1, blocking=False)\n",
    "        self._queue_rgb = self._device.getOutputQueue(\n",
    "            name='rgb_stream', maxSize=1, blocking=False)\n",
    "        self._queue_disparity = self._device.getOutputQueue(\n",
    "            name='disparity_stream', maxSize=1, blocking=False)\n",
    "\n",
    "    def release(self):\n",
    "        if self._device is not None:\n",
    "            self.loginfo('Releasing depthai device...')\n",
    "            try:\n",
    "                self._device.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.loginfo('depthai device released.')\n",
    "        self._device = None\n",
    "\n",
    "    def cv2jpeg_msg(self, cv_image, is_rgb):\n",
    "        if USE_CV_BRIDGE:\n",
    "            encoding = 'rgb8' if is_rgb else 'mono8'\n",
    "            return self._bridge.cv2_to_compressed_imgmsg(\n",
    "                cv_image, encoding=encoding, dst_format='jpeg')\n",
    "        elif is_rgb:\n",
    "            return rgb8_to_compressed_imgmsg(cv_image)\n",
    "        else:\n",
    "            return mono8_to_compressed_imgmsg(cv_image)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Image capture procedure.\n",
    "            Captures images from OAK-D and publishes them\n",
    "        \"\"\"\n",
    "        if (self._device is None) or (self._pipeline is None):\n",
    "            self.logerr('Device was not initialized!')\n",
    "            return\n",
    "\n",
    "        # keep reading\n",
    "        while (not self.is_stopped) and (not self.is_shutdown):\n",
    "            msgs = {'left': None, 'right': None, 'rgb': None, 'disparity': None}\n",
    "\n",
    "            if True: # self.image_left_pub.get_num_connections() > 0:\n",
    "                left = self._queue_left.get()\n",
    "                left = left.getCvFrame()\n",
    "                msgs['left'] = self.cv2jpeg_msg(left, is_rgb=False)\n",
    "\n",
    "            if True: # self.image_right_pub.get_num_connections() > 0:\n",
    "                right = self._queue_right.get()\n",
    "                right = right.getCvFrame()\n",
    "                msgs['right'] = self.cv2jpeg_msg(right, is_rgb=False)\n",
    "\n",
    "            if True: # self.image_rgb_pub.get_num_connections() > 0:\n",
    "                rgb = self._queue_rgb.get()\n",
    "                rgb = rgb.getCvFrame()\n",
    "                msgs['rgb'] = self.cv2jpeg_msg(rgb, is_rgb=True)\n",
    "\n",
    "            if True:\n",
    "                disparity = self._queue_disparity.get()\n",
    "                disparity = disparity.getFrame()\n",
    "                msgs['disparity'] = self.cv2jpeg_msg(disparity, is_rgb=False)\n",
    "\n",
    "            got = list(sorted([k for k, m in msgs.items() if m is not None]))\n",
    "            if len(got) > 0:\n",
    "              self.log('Got {} from OAK-D'.format(', '.join(got)))\n",
    "              self.publish(msgs)\n",
    "            else:\n",
    "              self.log('No images were read from OAK-D!')\n",
    "\n",
    "        self.loginfo('Camera worker stopped.')\n",
    "\n",
    "    def on_shutdown(self):\n",
    "        self.stop()\n",
    "\n",
    "    def srv_set_left_camera_info_cb(self, req):\n",
    "        self.log('[srv_set_left_camera_info_cb] Callback!')\n",
    "        return self.srv_set_camera_info_cb(req, 'left')\n",
    "\n",
    "    def srv_set_right_camera_info_cb(self, req):\n",
    "        self.log('[srv_set_right_camera_info_cb] Callback!')\n",
    "        return self.srv_set_camera_info_cb(req, 'right')\n",
    "\n",
    "    def srv_camera_info_cb(self, req):\n",
    "        self.log('[srv_camera_info_cb] Callback!')\n",
    "        return self.srv_set_camera_info_cb(req, 'rgb')\n",
    "\n",
    "    def srv_set_camera_info_cb(self, req, cam):\n",
    "        filename = self.cali_file_folder + rospy.get_namespace().strip('/') + ('_%s.yaml' % cam)\n",
    "        response = SetCameraInfoResponse()\n",
    "        response.success = self.save_camera_info(req.camera_info, filename, cam)\n",
    "        response.status_message = 'Wrote to %s' % filename\n",
    "        return response\n",
    "\n",
    "    def save_camera_info(self, camera_info_msg, filename, camera_name):\n",
    "        \"\"\"Saves intrinsic calibration to file.\n",
    "\n",
    "            Args:\n",
    "                camera_info_msg (:obj:`CameraInfo`): Camera Info containing calibration\n",
    "                filename (:obj:`str`): filename where to save calibration\n",
    "                camera_name (:obj:`str`): OAK-D camera name (left, right, rgb)\n",
    "        \"\"\"\n",
    "        # Convert camera_info_msg and save to a yaml file\n",
    "        self.log('[save_camera_info] filename: %s' % filename)\n",
    "\n",
    "        # Converted from camera_info_manager.py\n",
    "        calib = {\n",
    "            'image_width': camera_info_msg.width,\n",
    "            'image_height': camera_info_msg.height,\n",
    "            'camera_name': rospy.get_name().lstrip('/').split('/')[0] + ('_%s' % camera_name),\n",
    "            'distortion_model': camera_info_msg.distortion_model,\n",
    "            'distortion_coefficients': {\n",
    "                'data': camera_info_msg.D,\n",
    "                'rows': 1,\n",
    "                'cols': 5\n",
    "            },\n",
    "            'camera_matrix': {\n",
    "                'data': camera_info_msg.K,\n",
    "                'rows': 3,\n",
    "                'cols': 3\n",
    "            },\n",
    "            'rectification_matrix': {\n",
    "                'data': camera_info_msg.R,\n",
    "                'rows': 3,\n",
    "                'cols': 3\n",
    "            },\n",
    "            'projection_matrix': {\n",
    "                'data': camera_info_msg.P,\n",
    "                'rows': 3,\n",
    "                'cols': 4\n",
    "            }\n",
    "        }\n",
    "        self.log('[save_camera_info] calib %s' % calib)\n",
    "        try:\n",
    "            f = open(filename, 'w')\n",
    "            yaml.safe_dump(calib, f)\n",
    "            return True\n",
    "        except IOError:\n",
    "            return False\n",
    "\n",
    "    def update_camera_params(self, camera_name):\n",
    "        \"\"\" Update the camera parameters based on the current resolution.\n",
    "\n",
    "        The camera matrix, rectification matrix, and projection matrix depend on\n",
    "        the resolution of the image.\n",
    "        As the calibration has been done at a specific resolution, these matrices need\n",
    "        to be adjusted if a different resolution is being used.\n",
    "        \"\"\"\n",
    "        original_info = self.original_camera_infos[camera_name]\n",
    "        scale_width = float(self._res_w.value) / original_info.width\n",
    "        scale_height = float(self._res_h.value) / original_info.height\n",
    "\n",
    "        scale_matrix = np.ones(9)\n",
    "        scale_matrix[0] *= scale_width\n",
    "        scale_matrix[2] *= scale_width\n",
    "        scale_matrix[4] *= scale_height\n",
    "        scale_matrix[5] *= scale_height\n",
    "\n",
    "        # adjust the camera matrix resolution\n",
    "        self.current_camera_infos[camera_name].height = self._res_h.value\n",
    "        self.current_camera_infos[camera_name].width = self._res_w.value\n",
    "\n",
    "        # adjust the K matrix\n",
    "        self.current_camera_infos[camera_name].K = np.array(original_info.K) * scale_matrix\n",
    "\n",
    "        # adjust the P matrix\n",
    "        scale_matrix = np.ones(12)\n",
    "        scale_matrix[0] *= scale_width\n",
    "        scale_matrix[2] *= scale_width\n",
    "        scale_matrix[5] *= scale_height\n",
    "        scale_matrix[6] *= scale_height\n",
    "        self.current_camera_infos[camera_name].P = np.array(original_info.P) * scale_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def load_camera_info(filename):\n",
    "        \"\"\"Loads the camera calibration files.\n",
    "\n",
    "        Loads the intrinsic and extrinsic camera matrices.\n",
    "\n",
    "        Args:\n",
    "            filename (:obj:`str`): filename of calibration files.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`CameraInfo`: a CameraInfo message object\n",
    "\n",
    "        \"\"\"\n",
    "        with open(filename, 'r') as stream:\n",
    "            calib_data = yaml.safe_load(stream)\n",
    "        cam_info = CameraInfo()\n",
    "        cam_info.width = calib_data['image_width']\n",
    "        cam_info.height = calib_data['image_height']\n",
    "        cam_info.K = calib_data['camera_matrix']['data']\n",
    "        cam_info.D = calib_data['distortion_coefficients']['data']\n",
    "        cam_info.R = calib_data['rectification_matrix']['data']\n",
    "        cam_info.P = calib_data['projection_matrix']['data']\n",
    "        cam_info.distortion_model = calib_data['distortion_model']\n",
    "        return cam_info\n",
    "\n",
    "    @staticmethod\n",
    "    def default_intrinsics(camera_name):\n",
    "        calib = CameraInfo()\n",
    "        calib.width = 640\n",
    "        calib.height = 480\n",
    "        calib.K = [320.0, 0.0, 320.0,\n",
    "                   0.0, 240.0, 240.0,\n",
    "                   0.0, 0.0, 1.0]\n",
    "        calib.D = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        calib.R = [1.0, 0.0, 0.0,\n",
    "                   0.0, 1.0, 0.0,\n",
    "                   0.0, 0.0, 1.0]\n",
    "        calib.P = [240.0, 0.0, 320.0, 0.0,\n",
    "                   0.0, 240.0, 320.0, 0.0,\n",
    "                   0.0, 0.0, 1.0, 0.0]\n",
    "        return calib\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # initialize the node\n",
    "    camera_node = OAKDCameraNode()\n",
    "    camera_node.start()\n",
    "    # keep the node alive\n",
    "    rospy.spin()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
