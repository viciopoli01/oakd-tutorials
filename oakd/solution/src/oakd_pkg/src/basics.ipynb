{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: right\">\n",
    "  <img src=\"../images/dtlogo.png\" alt=\"Logo\" width=\"200\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’» 01 - OAKD basic integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import rospy\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import List, Dict, Callable, Optional, Any\n",
    "\n",
    "from sensor_msgs.msg import CompressedImage, Image\n",
    "\n",
    "import depthai as dai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline\n",
    "\n",
    "The \"pipeline\" is an object documented in the [`depthai` API](https://docs.luxonis.com/projects/api/en/latest/) and can be initialized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline() -> dai.Pipeline:\n",
    "    pipeline = dai.Pipeline()\n",
    "    pipeline.setOpenVINOVersion(version=dai.OpenVINO.Version.VERSION_2021_3)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and configuring nodes\n",
    "\n",
    "\"Nodes\" (not ROS nodes) are different types of sensing modalities of the OAK-D. For example: images, stereo, or neural networks. They are configured using a \"pipeline\" object as a reference, which tracks all of the nodes that get created and pass them onto the OAK-D hardware to configure the device internally once the structure is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(pipeline: dai.Pipeline) -> Dict[str, dai.Node]:\n",
    "    # LEFT CAMERA\n",
    "    cam_left = pipeline.createMonoCamera()\n",
    "    cam_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "    cam_left.setResolution(\n",
    "        dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "    # RIGHT CAMERA\n",
    "    cam_right = pipeline.createMonoCamera()\n",
    "    cam_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "    cam_right.setResolution(\n",
    "        dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "    # RGB CAMERA\n",
    "    cam_rgb = pipeline.createColorCamera()\n",
    "    cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "    cam_rgb.setResolution(\n",
    "        dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "    cam_rgb.setColorOrder(\n",
    "        dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "    cam_rgb.setInterleaved(False)\n",
    "    cam_rgb.setPreviewSize(640, 400)\n",
    "\n",
    "    # STEREO\n",
    "    stereo = pipeline.createStereoDepth()\n",
    "    stereo.setConfidenceThreshold(150)\n",
    "    stereo.setMedianFilter(\n",
    "        dai.StereoDepthProperties.MedianFilter.KERNEL_7x7)\n",
    "    stereo.setLeftRightCheck(False)\n",
    "    stereo.setExtendedDisparity(False)  # True = better for short-range\n",
    "    stereo.setSubpixel(False)  # True = better for long-range\n",
    "    stereo.setRectifyMirrorFrame(False)\n",
    "\n",
    "    return {\n",
    "        'left': cam_left,\n",
    "        'right': cam_right,\n",
    "        'rgb': cam_rgb,\n",
    "        'stereo': stereo\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "Output links are [a special type of node](https://docs.luxonis.com/projects/api/en/latest/components/nodes/xlink_out/) which exist at the interface of the OAK-D hardware and the interface of your computer or Duckiebot. They provide a way to access the data perceived by the OAK-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_links(pipeline: dai.Pipeline) -> Dict[str, dai.XLinkOut]:\n",
    "    xout_links = {\n",
    "        'left': pipeline.createXLinkOut(),\n",
    "        'right': pipeline.createXLinkOut(),\n",
    "        'rgb': pipeline.createXLinkOut(),\n",
    "        'disparity': pipeline.createXLinkOut()\n",
    "    }\n",
    "    for name, xout_link in xout_links.items():\n",
    "        xout_link.setStreamName(name)\n",
    "        xout_link.input.setBlocking(False)\n",
    "    return xout_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking nodes to each other\n",
    "\n",
    "The various inputs and outputs of nodes need to be linked together to define the data flow, i.e. _what_ goes _where_? This will be tracked internally by the \"pipeline\" object and later used to configure the OAK-D device. For example, for a stereo node we need to define the two input images. For the output nodes, they need to know which \"upstream\" node in the computational graph they should be receiving data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_nodes_and_outputs(nodes: Dict[str, dai.Node],\n",
    "                           outputs: Dict[str, dai.XLinkOut]) -> None:\n",
    "    # Raw RGB image --> Image manipulation module\n",
    "    nodes['rgb'].preview.link(outputs['rgb'].input)\n",
    "    # Disparity --> link the raw left & right images as input to stereo\n",
    "    nodes['left'].out.link(nodes['stereo'].left)\n",
    "    nodes['right'].out.link(nodes['stereo'].right)\n",
    "    nodes['stereo'].disparity.link(outputs['disparity'].input)\n",
    "    nodes['stereo'].rectifiedLeft.link(outputs['left'].input)\n",
    "    nodes['stereo'].rectifiedRight.link(outputs['right'].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a device\n",
    "\n",
    "Once all of the nodes are initialized, configured, and properly linked together, the device can be created. The computer or Duckiebot will try to search for an OAK-D device using the `depthai` library and to configure the hardware using our \"pipeline\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_device(pipeline: dai.Pipeline) -> dai.Device:\n",
    "    device = dai.Device(pipeline, usb2Mode=False)\n",
    "    device.setLogLevel(dai.LogLevel.DEBUG)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output queues\n",
    "\n",
    "Remember those special \"output links\"? In order to read them, we need to initialize some \"output queues\" which will link to those outputs on the computer or Duckiebot side of things.\n",
    "\n",
    "If a queue is \"blocking\", when a queue is full, any new incoming data will be dropped from the queue until the first item is read (popped) from the queue and a new space gets freed up. In contrast, a non-blocking queue will always insert new incoming data into the queue, and if the queue is filled up, the oldest data will get dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_queues(device: dai.Device,\n",
    "                         outputs: Dict[str, dai.XLinkOut]) -> Dict[str, dai.DataOutputQueue]:\n",
    "    queues = dict()\n",
    "    for name, xout_link in outputs.items():\n",
    "        queues[name] = device.getOutputQueue(name=name, maxSize=1, blocking=False)\n",
    "    return queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "With everything set up, let's create a utility function to read some specific requested data from the OAK-D, and return it in a dictionary mapping from the data name to the raw data (e.g. a Numpy array containing image data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(requested_data: List[str],\n",
    "              output_queues: Dict[str, dai.DataOutputQueue]) -> Dict[str, Any]:\n",
    "    data = {name: None for name in requested_data}\n",
    "    for name in requested_data:\n",
    "        if name not in output_queues.keys():\n",
    "            continue\n",
    "        if name in ('left', 'right', 'rgb'):\n",
    "            data[name] = output_queues[name].get().getCvFrame()\n",
    "        elif name == 'disparity':\n",
    "            data[name] = output_queues[name].get().getFrame()\n",
    "    received = ', '.join(list(k for k, d in data.items() if d is not None))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualize\n",
    "\n",
    "Here is a simple helper function which uses the functions we've written to set up the OAK-D, continuously read data, and visualize it. This function optionally includes an \"image processing\" function which post-processes the data that we will read. This will come in handy for visualizing disparity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "ImageProcFunc = Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "def hello_oakd(output_name: str,\n",
    "               im_proc: Optional[ImageProcFunc]=None) -> None:\n",
    "    display_handle = display(None, display_id=True)\n",
    "    pipeline = create_pipeline()\n",
    "    nodes = create_nodes(pipeline)\n",
    "    outputs = create_output_links(pipeline)\n",
    "    link_nodes_and_outputs(nodes, outputs)\n",
    "    device = create_device(pipeline)\n",
    "    output_queues = create_output_queues(device, outputs)\n",
    "    i = 0\n",
    "    try:\n",
    "        while True:\n",
    "            i += 1\n",
    "            data = read_data([output_name], output_queues)\n",
    "            if output_name in data.keys():\n",
    "                frame = data[output_name]\n",
    "                if im_proc is not None:\n",
    "                    frame = im_proc(frame)\n",
    "                _, frame = cv2.imencode('.jpeg', frame)\n",
    "                display_handle.update(Image(data=frame.tostring()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        pass\n",
    "        # display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "\n",
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - color camera\n",
    "cam_rgb = pipeline.createColorCamera()\n",
    "cam_rgb.setPreviewSize(300, 300)\n",
    "cam_rgb.setBoardSocket(dai.CameraBoardSocket.RGB)\n",
    "cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Create output\n",
    "xout_rgb = pipeline.createXLinkOut()\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "display_handle = display(None, display_id=True)\n",
    "print(\"ciao\")\n",
    "# Pipeline defined, now the device is connected to\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Start pipeline\n",
    "    device.startPipeline()\n",
    "    print(\"ciao1\")\n",
    "    # Output queue will be used to get the rgb frames from the output defined above\n",
    "    q_rgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        print(\"ciao2\")\n",
    "        in_rgb = q_rgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "        # Retrieve 'bgr' (opencv format) frame\n",
    "        _, frame = cv2.imencode('.jpeg',  in_rgb.getCvFrame())\n",
    "        display_handle.update(Image(data=frame.tostring()))\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_721/3809181025.py:26: DeprecationWarning: setConfidenceThreshold() is deprecated, Use 'initialConfig.setConfidenceThreshold()' instead\n",
      "  stereo.setConfidenceThreshold(150)\n",
      "/tmp/ipykernel_721/3809181025.py:27: DeprecationWarning: setMedianFilter() is deprecated, Use 'initialConfig.setMedianFilter()' instead\n",
      "  stereo.setMedianFilter(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to find device (ma2480), error message: X_LINK_DEVICE_NOT_FOUND",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/ipykernel_721/4284622853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhello_oakd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ipykernel_721/3186448680.py\u001b[0m in \u001b[0;36mhello_oakd\u001b[0;34m(output_name, im_proc)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_output_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlink_nodes_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moutput_queues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_output_queues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipykernel_721/3591016613.py\u001b[0m in \u001b[0;36mcreate_device\u001b[0;34m(pipeline)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musb2Mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to find device (ma2480), error message: X_LINK_DEVICE_NOT_FOUND"
     ]
    }
   ],
   "source": [
    "hello_oakd('rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_oakd('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_oakd('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disparity_to_rgb(disparity: np.ndarray) -> np.ndarray:\n",
    "    norm = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    colored = cv2.applyColorMap(norm, cv2.COLORMAP_JET)\n",
    "    return colored\n",
    "\n",
    "hello_oakd('disparity', im_proc=disparity_to_rgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
